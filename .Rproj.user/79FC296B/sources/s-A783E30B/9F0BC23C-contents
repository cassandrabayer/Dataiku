---
title: "Modeling and Analysis"
author: "Cassandra Bayer"
date: "6/14/2018"
output: html_document
---
Source the controller, wherein necessary packages and custom functions get read in. The controller also 
reads in the data and uses a cleaning function to get it in a place that we can work with it. I then separate 
out my cleaned data and prepare to work with it.

```{r}
source("controller.R")
full_train <- census_train
full_test <- census_test

census_train <- census_train$dtClean
census_test <- census_test$dtClean
```

Before diving in, I'm curious to see the distribution of my data as well as the distribution of the older, messier data.
Everyone likes an ego boost. After running the summary, since most of my variables are dummies, nothing is too surprising.
I then compare it to the data as it was. As I suspeced, the qualitative data is not ideal for summary statistics or analysis.
However, despite my cleaning,  the summary is not incredibly helpful. I then make a quick set of barcharts
to view the distribution of the data. One could also facet wrap and use ggplot functionality; however, since the graphs are
only for personal eye-balling, base R graphics do the job.
```{r}
summary(census_train)
summary(full_train$dtMessy)
meltMissing <- full_train$missingPct
barchart <- barchart(meltMissing)
```

I then do the same at-a-glance review, but a bit differently for the continuous data. Here, rather than raw
frequencies, I can see a better picture of the distribution of the data.
```{r}
# First I want to get a basic summary of my data. However, since my custom data is mostly binary, this is not 
# helpful, nor intuitive. So I do that only for my integer data types.
intData <- select_if(census_train, is.integer)
summary(intData)
boxplot(intData)
```

Lastly, I put it all together in one place. This serves as a gut check to the graphs and summaries I was seeing above.
```{r}
# For the rest of the data, I was to get an idea of frequencies.
meltedCensus <- melt(census_train)
ggplot(meltedCensus,aes(x = value)) + 
    facet_wrap(~variable,scales = "free_x") + 
    geom_histogram()

```

First, I want to look at the missing data in the original data; again, many of the columns I have decided to 
omit were riddled with missing data. Then, I want to get a sense of any missing data still remaining in the "clean" data I created.
Using a standard `missmap`, I organize my data by any outstanding missing data. It doesn't look too bad, but
I can still weed out any remaining missing values. Although I could impute using the mean, the data is so large
that the missing records be dropped should have a negligible effect. With more time, I would like to test the 
effect of dropping missing records.
```{r}
meltedMissing <- melt(full_train$missingPct)
barchart(full_train$missingPct)
ggplot(meltedMissing,aes(x = value)) + 
    facet_wrap(~variable,scales = "free_x") + 
    geom_histogram()
missmap(census_train, col=c("blue", "red"), legend=FALSE)

```

Now I want to get a sense of how my variables are correlated. I suspect some covariance, but that's expected
given the nature of the dummy variables (where sex is female, it cnanot also be male; I would anticipate those
to be perfectly inversely related). However, I'm interested to see if any of these variables correlate with 
variables outside of their binary counterparts.I'm seeing some interesting relationships, so I'd like to explore 
them more using some basic regression.
```{r}
corr <- cor(census_train)
corrplot(corr, method="circle")
```


```{r}
# source shiny to show basic relationships
source("shinyController")

```

Create a model using these variables (you can use whichever variables you want, or even create you own; for example, you could find the ratio or relationship between different variables, the one-hot encoding of “categorical” variables, etc.) to model wining more or less than $50,000 / year. Here, the idea would be for you to test one or two algorithms, such as logistic regression, or a decision tree. Feel free to choose others if wish.

First, I run a simple logistic regression using all the variables in my arsenal. The output shows me that,
given my model, only 347 respondents make over 50k and the rest under 50k. Because I have labeled data, I know
the rate to be much higher. As it stands, the proportion of those making over 50k is about 6%, whereas my model
predicts less than 1%. There is some work to be done.
```{r}
lm <- glm(data = census_train,formula =  over50k ~ ., family=binomial(link= "logit"))
summary(lm)
plot(lm$fitted.values)
plot(lm$residuals)
glm.probs <- predict(lm,type = "response")
glm.pred <- ifelse(glm.probs > 0.5, "Over 50k", "Under 50k")
table(glm.pred)
```

Using a stepwise model, I find the ideal formula, which selects only the variables of importance. I find that 
ageSq, male, normalizedWageHr, foreignDad, wksWorkedHowever,
since there are still many variables, I suspect overfitting. In fact, running a quick test for false R^2s, my 
McFadden test shows that there is little difference between the two models that I have constructed. To the ROC curve!
```{r}
step.model <- stepAIC(lm, direction = "both", 
                      trace = FALSE)
summary(step.model)

lm2 <- glm(formula = over50k ~ ageSq + male + normalizedWageHr + foreignDad + 
    wksWorkedPastYr + black + unemployed + belowCollege + college + 
    aboveMasters + married + householder + bothParents + blackDivorcedM + 
    hispanicDivorcedM, data = census_train, family = binomial(link = "logit"))

anova(lm)
anova(lm2)
pR2(lm2)
pR2(lm)
```

```{r}
predResults <- predict(lm2, newdata=census_test,type='response')
predResults <- ifelse(predResults > 0.5,1,0)

misClasificError <- mean(predResults != census_test$over50k, na.rm = T)
print(paste('Accuracy',1-misClasificError))

```

https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/
```{r}
performance <- prediction(predResults, census_test$over50k)
prf <- performance(performance, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(performance, measure = "auc")
auc <- auc@y.values[[1]]
auc


```
But what about an SVM? https://www.r-bloggers.com/machine-learning-using-support-vector-machines/
```{r}
library(e1071)
 
#Fit a model. The function syntax is very similar to lm function
svm <- svm(over50k ~ ageSq + male + normalizedWageHr + foreignDad + 
    wksWorkedPastYr + black + unemployed + belowCollege + college + 
    aboveMasters + married + householder + bothParents + blackDivorcedM + 
    hispanicDivorcedM, data = census_train)
 
#Use the predictions on the data
svmPred <- predict(svm, census_train)

svm_roc <- prediction(svmPred, census_test$over50k)

```

Compare errors between logit and svm
```{r}
error_lm <- lm2$residuals
 
lm2_error <- sqrt(mean(error_lm^2)) # 15.7
 
#For svm, we have to manually calculate the difference between actual values (train$y) with our predictions (pred)
 
error_svm <- census_train$over50k - svmPred
 
svm_error <- sqrt(mean(error_svm^2)) #.22 

## could also tune
```

Lastly, a decision tree http://trevorstephens.com/kaggle-titanic-tutorial/r-part-3-decision-trees/
```{r}

tree <- tree(over50k ~ ageSq + male + normalizedWageHr + foreignDad + 
    wksWorkedPastYr + black + unemployed + belowCollege + college + 
    aboveMasters + married + householder + bothParents + blackDivorcedM + 
    hispanicDivorcedM, data = census_train, method = "class")
summary(tree)

pred <- predict(tree, newdata=census_test)

roc_pred <- prediction(pred, census_test$over50k)
plot(performance(roc_pred, measure="tpr", x.measure="fpr"), colorize=TRUE)

```

Choose the model that appears to have the highest performance based on a comparison between reality (the 42nd variable) and the model’s prediction. 
```{r}
```

Apply your model to the test file and measure it’s real performance on it (same method as above).
```{r}


```


Find clear insights on the profiles of the people that make more than $50,000 / year. For example, which variables seem to be the most correlated with this phenomenon?
```{r}


```

Describe steps and methodology
```{r}

```

Describe challenges
```{r}

```

