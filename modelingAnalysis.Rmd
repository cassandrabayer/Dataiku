---
title: "Modeling and Analysis"
author: "Cassandra Bayer"
date: "6/15/2018"
output: html_document
---
Source the controller, wherein necessary packages and custom functions get read in. The controller also 
reads in the data and uses a cleaning function to get it in a place that we can work with it. I then separate 
out my cleaned data and prepare to work with it.

```{r}
source("controller.R")
full_train <- census_train
full_test <- census_test

census_train <- census_train$dtClean
census_test <- census_test$dtClean
```

Before diving in, I'm curious to see the distribution of my data as well as the distribution of the older, messier data.
Everyone likes an ego boost. After running the summary, since most of my variables are dummies, nothing is too surprising.
I then compare it to the data as it was. As I suspeced, the qualitative data is not ideal for summary statistics or analysis.
However, despite my cleaning,  the summary is not incredibly helpful. I then make a quick set of barcharts
to view the distribution of the data. One could also facet wrap and use ggplot functionality; however, since the graphs are
only for personal eye-balling, base R graphics do the job.
```{r}
summary(census_train)
summary(full_train$dtMessy)
meltMissing <- full_train$missingPct
barchart <- barchart(meltMissing)
```

I then do the same at-a-glance review, but a bit differently for the continuous data. Here, rather than raw
frequencies, I can see a better picture of the distribution of the data.
```{r}
# First I want to get a basic summary of my data. However, since my custom data is mostly binary, this is not 
# helpful, nor intuitive. So I do that only for my integer data types.
intData <- select_if(census_train, is.integer)
summary(intData)
boxplot(intData)
```

Lastly, I put it all together in one place. This serves as a gut check to the graphs and summaries I was seeing above.
```{r}
# For the rest of the data, I was to get an idea of frequencies.
meltedCensus <- melt(census_train)
ggplot(meltedCensus,aes(x = value)) + 
    facet_wrap(~variable,scales = "free_x") + 
    geom_histogram()

```

First, I want to look at the missing data in the original data; again, many of the columns I have decided to 
omit were riddled with missing data. Then, I want to get a sense of any missing data still remaining in the "clean" data I created.
Using a standard `missmap`, I organize my data by any outstanding missing data. It doesn't look too bad, but
I can still weed out any remaining missing values. Although I could impute using the mean, the data is so large
that the missing records be dropped should have a negligible effect. With more time, I would like to test the 
effect of dropping missing records.
```{r}
meltedMissing <- melt(full_train$missingPct)
barchart(full_train$missingPct)
ggplot(meltedMissing,aes(x = value)) + 
    facet_wrap(~variable,scales = "free_x") + 
    geom_histogram()
missmap(census_train, col=c("blue", "red"), legend=FALSE)

```

Now that we know what's missing, let's drop missing values.
```{r}
census_train <- na.omit(census_train)
census_test <- na.omit(census_test)
```

Now I want to get a sense of how my variables are correlated. I suspect some covariance, but that's expected
given the nature of the dummy variables (where sex is female, it cnanot also be male; I would anticipate those
to be perfectly inversely related). However, I'm interested to see if any of these variables correlate with 
variables outside of their binary counterparts.I'm seeing some interesting relationships, so I'd like to explore 
them more using some basic regression.
```{r}
corr <- cor(census_train)
corrplot(corr, method="circle")
```

This gives me a playground to do more work with variable selection. Given the time, this would be one technique
I'd use as a gut check for feature selection when building a model.
```{r}
runApp()

```

First, I run a simple logistic regression using all the variables in my arsenal. The output shows me that,
given my model, only 347 respondents make over 50k and the rest under 50k. Because I have labeled data, I know
the rate to be much higher. As it stands, the proportion of those making over 50k is about 6%, whereas my model
predicts about 2%. There is some work to be done. For this model, because I believe it to be biased, I don't 
waste time constructing a ROC curve to test it's predictive power.
```{r}
lm <- glm(data = census_train,formula =  over50k ~ ., family=binomial(link= "logit"))
summary(lm)

probs <- predict(lm,type = "response")
pred <- ifelse(probs > 0.5, "Over 50k", "Under 50k")
table(pred)
```

Using a stepwise model, I find the ideal formula given the variables that I have, which selects only the variables of importance.
Since there are still many variables, I suspect overfitting. I would also explore the "Leave One Out" step to
find the optimal point at which my model had enough predictive power before introducing variable covariance.
```{r}
# I've commented this out becuase it is so performance intensive. However, I have used the call it returns 
# below in lm2.
# step.model <- stepAIC(lm, direction = "both", 
#                       trace = FALSE)
# summary(step.model)

lm2 <- glm(formula = over50k ~ ageSq + male + normalizedWageHr + foreignDad + 
    wksWorkedPastYr + black + unemployed + belowCollege + college + 
    aboveMasters + married + householder + bothParents + blackDivorcedM + 
    hispanicDivorcedM, data = census_train, family = binomial(link = "logit"))

summary(lm2)
```

In fact, running a quick test for false R^2s, my McFadden test shows that there is little difference between 
the two models that I have constructed. I also run an likelihood ratio test to check the goodness of fit between the two models.
At this point, I've already discounted lm1, but I use it as a baseline from which to compare lm2.
```{r}
pR2(lm2)
pR2(lm)

anova(lm, lm2, test="LRT")

```

For better or for worse, I've decided to stick with lm2. At this point, I'm curious about it's real predictive 
power. So, at this point, I apply the model to my test data. I extract the classification error from my model 
to back out its accuracy. I'm pleasantly surprised that the model has 94.7% accuracy.
```{r}
predResults <- predict(lm2, newdata=census_test,type='response')
predResults <- ifelse(predResults > 0.5,1,0)

classificationError <- mean(predResults != census_test$over50k, na.rm = T)
print(paste('Accuracy',1-classificationError))

```

I'm curious to now get a visual of performance and it's area under the ROC. The model looks okay, but not great; 
it has moderate sensitivity and specificity. 
```{r}
performance <- prediction(predResults, census_test$over50k)
prf <- performance(performance, measure = "tpr", x.measure = "fpr")
plot(prf)

auc <- performance(performance, measure = "auc")
auc <- auc@y.values[[1]]
auc


```

However, I wonder if I could do better. Naturally, I turn to SVM, or what I deem to be to the closest thing
to wizardry I'll ever perform. I use the same variables in my model as my call from the step model.
```{r}
svm <- svm(over50k ~ ageSq + male + normalizedWageHr + foreignDad + 
    wksWorkedPastYr + black + unemployed + belowCollege + college + 
    aboveMasters + married + householder + bothParents + blackDivorcedM + 
    hispanicDivorcedM, data = census_train)
svmPred <- predict(svm, census_train)
svmPrediction <- prediction(svmPred, census_test$over50k)

```

Now I want to compare the errors between my first model and ny svm model. The errors from the svm are naturally
much smaller. This is intuitive because SVM is designed to handled irregularly distributed data as well as data
with higher dimensionality.
```{r}
error_lm <- lm2$residuals
lm2_error <- sqrt(mean(error_lm^2))
 
error_svm <- census_train$over50k - svmPred
svm_error <- sqrt(mean(error_svm^2))  
table(svm_error, lm2_error)

```

Lastly, out of curiosity, I construct a decision tree.
```{r}

tree <- tree(over50k ~ ageSq + male + normalizedWageHr + foreignDad + 
    wksWorkedPastYr + black + unemployed + belowCollege + college + 
    aboveMasters + married + householder + bothParents + blackDivorcedM + 
    hispanicDivorcedM, data = census_train, method = "class")
summary(tree)

pred <- predict(tree, newdata=census_test)

roc_pred <- prediction(pred, census_test$over50k)
plot(performance(roc_pred, measure="tpr", x.measure="fpr"), colorize=TRUE)

```

## Analysis and final write up
Ultimately, I would have chosen the SVM model barring complications and more time. However, given that the basic
logistic model had high prediciton power, I feel confident, in this context choosing it. The results of the regression 
are helpful to back out meaningful insights about those earning below and above $50, 000 annually.

The error is significantly lower than the error that I found in my second logistic model. And while the tree did not perform terribly, the high dimensionality in the data may
not suit the model well.


## Challenges
Most of my challenges were time based. Typically, I would spend much longer structuring the data differently 
for different models. Accordingly, I was not being as keen to watch out for bias in my models, which was certainly present. However, the typical
bias-variance tradeoff reared its ugly head, and I decided to err more towards capturing explanatory power versus
it being strictly accurate in this case; that was something out of character and honestly difficult to get past.
I also experienced challenges up front when cleaning the data. Because the data was messy and primarily quantitative,
I weighed considerations between factorizing the data or making a series of dummy variables.

I would have also liked to create more interaction variables. I know that the variables to not meet the assumption
of independence and thusly muddy up the findings. In hindsight, and given more time, I would have constructed the 
model very differently. However, as an explatory first pass I was interested in the findings.


